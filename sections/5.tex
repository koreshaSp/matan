\section*{Лекция 5 (17.03.2022)}
\section{Кривые и пути}
\subsection{Вводная в кривые}
\begin{definition}
    Путь в $\R^m, \gamma : [a, b] \mapsto \R^m$~--- непрерывное отображение из отрезка в $\R^m$.
\end{definition}
\begin{definition}
    Носителем пути или кривой назовём следующее множество точек: $\gamma\left([a, b]\right)\in \R^m$
\end{definition}
\begin{definition}
    Путь замкнут если $\gamma(a) = \gamma(b)$. Иными словами путь замкнут, если начало пути совпадает с концом.
\end{definition}
\begin{definition}
    Если $\gamma$~--- инъекция, то такой путь называется
    простым.
\end{definition}
\begin{remark}
    Можно определять путь покоординатно:
    $\gamma(t) = (\gamma_1(t),\dots, \gamma_m(t))$, где $\gamma_i$~--- координатные функции.
    $\gamma$ --- непр. $\Longleftrightarrow$ все $\gamma_j$ --- непр.
\end{remark}

\begin{definition}
    Назовём два пути $\gamma_1\colon [a,b]\mapsto \R^m$ и
    $\gamma_2\colon [c,d]\mapsto \R^m$ эквивалентными,\\
    если $\exists \text{строго возрастающая биекция }
    \varphi\colon [a,b]\mapsto[c, d]$, так что 
    $\gamma_2\circ \varphi = \gamma_1$.
\end{definition}
\begin{remark}
    Определение эквивалентности путей является корректным отношением эквивалентности для пространства путей в $\R^m$.

    Можно воспринимать это следующим образом.
    Кривая в $\R_m$~--- класс эквивалентности путей. Представители~--- параметризация отрезком.
\end{remark}
\begin{example}
    У носителя пути есть различные параметризации:
    $(\cos x, \sin x), (\sqrt{1 - y^2}, y)$(полуокружность)
\end{example}
\begin{remark}
    Зная параметризацию можно определить носитель кривой,
    начало и конец, но хочется понимать ещё про гладкость.
\end{remark}
\begin{definition}
    $\gamma$ ~--- гладкий путь, если $\gamma_i\in C^1[a,b]$.
\end{definition}
\begin{definition}
    Гладкая кривая~--- кривая, у которой $\exists$ гладкий путь.
\end{definition}
\begin{remark}
    Нельзя утверждать, что у гладкой кривой есть явная касательная
    в каждой её точке.
\end{remark}
\subsection{Длина кривой}
По факту, хотим подробить нашу кривую на маленькие отрезочки
и просуммировать их, давайте подумаем как это сделать аккуратно.\\
\quad
\begin{definition}
    $\gamma\colon [a, b]\mapsto \R^m$~--- путь.
    $\theta = \{t_0,\dots, t_n\}, a = t_0 < t_1 < \dots < t_n = b$~---
    разбиение $[a, b]$.
    \[
        l_\theta(\gamma) = \sum\limits_{j = 1}^{n} \abs{\gamma(t_j) - \gamma(t_{j - 1})};\\
    \]
    Тогда длиной назовём  $l(\gamma) = \sup_\theta l_\theta(\gamma)$
\end{definition}
\begin{remark}
    Заметим, что длина есть у любого пути, так как мы не вводили
    дополнительные требования на путь.
\end{remark}
\begin{remark}
    Посмотрим на $(x, x\sin \frac{1}{x})$
    Вопрос: Путь бесконечной длины?
\end{remark}
\begin{theorem}[Независимость длины от параметризации]
    $\gamma_1 \sim \gamma_2\Rightarrow l(\gamma_1) = l(\gamma_2)$
\end{theorem}
\begin{proof}
    $\theta$~--- дробление $[a,b]$,
    $\varphi(\theta)$~--- дробление $[c,d]$.
    $\gamma_2\circ\varphi=\gamma_1$.
    \[
        \sum\limits_{j = 1}^{n}\abs{\gamma_1(t_j) - \gamma_1(t_{j-1})}=\\
        \sum\limits_{j=1}^{n}\abs{\gamma_2(\varphi(t_j))-\gamma_2(\varphi(t_{j-1}))}
        \Rightarrow
        l_{\theta}(\gamma_1) = l_{\varphi(\theta)}(\gamma_2)
    \]
\end{proof}
\begin{theorem}[Аддитивность длины]
    $\gamma\colon [a,b]\mapsto\R^m, c\in(a,b)$.
    Пусть $\gamma_{-}=\gamma\big|_{[a,c]}, \gamma_{+} = \gamma\big|_{[c,b]}$.
    Требуется доказать, что $l(\gamma)=l(\gamma_-) + l(\gamma_+)$
\end{theorem}
\begin{proof}
    Докажем неравенство в две стороны.
    \begin{itemize}
        \item
        Пусть $\theta = \{x_1,\dots, x_n\}$~--- произвольное дробление $[a,b]$.
        Выберем за $k$ следующее число: $x_k \in [a,c], x_{k + 1}\in [c,b]$.
        Разделим наше дробления на два:
        $\theta_+ = (\theta \cap [a,c]) \cup \{c\} = \{x_1,\dots,x_k, c\},
        \theta_- = \{c\} \cup (\theta \cap [c, b]) = \{c, x_{k+1},\dots, x_n\}$.
        \[
            \begin{gathered}
                l_{\theta}(\gamma) =
                \sum\limits_{i=1}^{n}\norm{\gamma(x_i) - \gamma(x_{i + 1})}=
                \sum\limits_{i=1}^{k-1}\norm{\gamma(x_i) - \gamma(x_{i + 1})} + \norm{x_k - x_{k+1}} +
                \sum\limits_{i=k+1}^{n-1}\norm{\gamma(x_i) - \gamma(x_{i + 1})} \leqslant\\
                \leqslant \sum\limits_{i=1}^{k-1}\norm{\gamma(x_i) - \gamma(x_{i + 1})}
                + \norm{x_k - c} + \norm{c - x_{k + 1}} +
                \sum\limits_{i=k+1}^{n-1}\norm{\gamma(x_i) - \gamma(x_{i + 1})}=
                l_{\theta_-}(\gamma) + l_{\theta_+}(\gamma)
            \end{gathered}
        \] 
        Так как было взята произвольная $\theta$, то из этого следует и оценка на $l(\gamma) = \sup_\theta(\dots)$.
        \[
            l(\gamma) \le l(\gamma_+) + l(\gamma_-)
        \] 
        \item
        Докажем в обратную сторону.
        Пусть $\theta_-, \theta_+$~--- дробления $[a,c], [c, b]$ соответственно.
        Тогда определим $\theta = \theta_- \cup \theta_+$. Отсюда, по определению длины следует неравенство:
         \[
             l(\gamma) = \sup_{\tau}(l_{\tau}(\gamma)) \ge l_{\theta}(\gamma) = l_{\theta_-}(\gamma) + l_{\theta_+}(\gamma)
        \] 
        Так как были взяты произвольные $\theta_-, \theta_+$, то можно утверждать, что:
         \[
             l(\gamma) \ge l(\gamma_+) + l(\gamma_-)
        \] 
    \end{itemize}
\end{proof}
\begin{theorem}[O длине гладкого пути]
    $\gamma\colon[a,b]\mapsto\R^m, \gamma_j\in C^1[a,b] \Rightarrow 
    l(\gamma)=\bigintss_{a}^{b}\abs{\gamma'(t)}dt$, где
    $\abs{\gamma'(t)} = \sqrt{\sum\limits_{j=1}^{m}{\abs{\gamma'_j(t)}}^2}$
\end{theorem}
\begin{proof}
    $\theta$ ~--- дробление $[a,b]$,  
    $\theta\colon a = t_0<\dots<t_n=b$ 
    $$l_{\theta}(\gamma)=\sum\limits_{j=1}^{m}\abs{\gamma(t_j) - \gamma(t_{j-1})}$$
     \[
         \abs{\gamma(t_j)-\gamma(t_{j-1})}=
         \sqrt{\sum\limits_{k=1}^{m}\abs{\gamma_k(t_j)-\gamma_k(t_{j-1})}^2}=
         \sqrt{\sum\limits_{k=1}^{m}{\abs{\gamma'_k(?)}^2(t_j-t_{j-1})^2}}=
         (t_j - t_{j-1})\sqrt{\sum\limits_{k=1}^{m}{\abs{\gamma'_k(?)}^2}}
     .\] 
     Пусть $m_{j,k}=\min\limits_{[t_{j-1},t_j]} \abs{\gamma_k'}$,
     $M_{j,k}=\max\limits_{[t_{j-1},t_j]}\abs{\gamma'_k}$.
     Тогда можно продолжить  следующим образом:
     $$(t_j - t_{j-1})\sqrt{\sum\limits_{k=1}^{m}{m_{j,k}^2}}\leqslant
     \abs{\gamma(t_j) - \gamma(t_{j-1})}\leqslant
     (t_j - t_{j-1})\sqrt{\sum\limits_{k=1}^{m}{M_{j,k}^2}}$$
     С другой стороны:
     $$(t_j - t_{j-1})\sqrt{\sum\limits_{k=1}^{m}{m_{j,k}^2}}\leqslant
     \int\limits_{t_{j-1}}^{t_j}{\abs{\gamma'(t)}dt}\leqslant
     (t_j - t_{j-1})\sqrt{\sum\limits_{k=1}^{m}{M_{j,k}^2}}$$
     Суммируем по $j$:
     \[
         \sum\limits_{}^{}{(t_j-t_{j-1})\sqrt{?}}\leqslant
         \begin{cases}
             l_{\theta}(\gamma)\\
             \int\limits_{a}^{b}{\abs{\gamma'(t)}dt}
         \end{cases}\leqslant
         \sum\limits_{}^{}{(t_j-t_{j-1})\sqrt{?}}
     .\] 
     Заметим, что обе части стремятся к одному и тоже при
     $\abs{\theta}\rightarrow 0$, так как в этом случае $m_{j,k}\approx M_{j,k}$.

     Тогда для  $\varepsilon > 0$ по теореме кантора $\delta > 0\colon\\
     \forall s,t\in [a,b]\colon \abs{s - t} < \delta, 
     \forall k = 1\dots m, \abs{\gamma'_k(s)-\gamma'_k(t)} < \varepsilon$ 
     Тогда $\abs{M_{j,k}-m_{j,k}}<\varepsilon$ если $t_j - t_{j-1}<\delta$.
     \[
         \sqrt{\sum\limits_{k=1}^{m}{M^2_{j,k}}}-
         \sqrt{\sum\limits_{k=1}^{m}{m^2_{j,k}}} \leqslant
         \sqrt{\sum\limits_{k=1}^{m}{(M_{j,k} - m_{j,k})^2}}<
         \sqrt{m}\varepsilon
     .\] 
     Тогда $$\abs{\text{п.ч - л.ч}} <
     \sum\limits_{k=1}^{m}{(t_j-t_{j-1})\sqrt{m}\varepsilon}=
     (b - a)\sqrt{m}\varepsilon$$
\end{proof}
\begin{follow}
    \begin{enumerate}
        \item (Длина графика функции)\\
            Пусть $f\colon[a,b]\mapsto \R, f\in C^1[a,b], 
            \Gamma_f=\{(x,f(x)) \mid x\in [a,b]\}\subseteq \R^2$.
            Тогда длина графика функции равна:
            $l(\Gamma_f)=\bigintss\limits_{a}^{b}{\sqrt{1 + (f'(x))^2}dx}$
        \item
            Есть функция 
            $r\colon [\alpha, \beta]\mapsto \R_+, r\in C^1[\alpha, \beta]$.
            Тогда можно запараметризовать эту кривую как:
            $(r(t)\cos t, r(t)\sin t)$, обозначим за координаты
            $x(t), y(t)$ соответственно.

            Тогда:
             $x'(t) = r'(t)\cos t - r(t)\sin t; 
             y'(t) = r'(t)\sin t + r(t)\cos t$

             Значит длину кривой можно вычислить следующим образом:
             \[
                 \begin{gathered}
                     l = 
                     \int\limits_{\alpha}^{\beta}{\sqrt{(x'(t))^2 + (y'(t))^2}}=\\
                     =\int\limits_{\alpha}^{\beta}{\sqrt{(\cos^2 t + \sin^2 t)(r'(t))^2 + (\cos^2 t + \sin^2 t)r^2(t)}\ dt}=\\
                     =\int\limits_{\alpha}^{\beta}{\sqrt{r'(t)^2 + r(t)^2}\ dt}
                 \end{gathered}
             \]
    \end{enumerate}
\end{follow}
\section{Линейные и полилинейные отображения}
\begin{remark}
    Прежде чем начать, подумаем что нам вообще надо.
    Производную можно представлять как линейное приближение
    функции в какой-либо точке.
    $f\colon X\mapsto Y$, где $X,Y$~--- линейные, нормированные,
    полные, пространства над одним полем
    скаляров($\R$ или $\mathbb{C}$).
    $f(x) = f(x_0) + A(x-x_0) + o(x - x_0)$.
    Объясним почему наложены те или иные условия.
    В одномерье $A$ было числом. В общем случае $A$ должно
    быть линейным отображением, поэтому нам хочется чтобы поле
    скаляров у $X,Y$ было одним и тем же.
    Более того, хочется иметь возможность переходить к пределу,
    а когда мы переходим к пределу естественно хотеть иметь
    корректное расстояние, поэтому нам нужна нормированность и полнота.
\end{remark}
\subsection{Основные свойства и определения}
\begin{definition}
    $X, Y$ ~--- линейные пространства над одним полем скаляров.
    $U\colon X\mapsto Y$ ~--- линейное, если
    \begin{enumerate}
        \item (Аддитивность)\\
            $U(x_1 + x_2)=U(x_1)+U(x_2)$
        \item (Однородность)\\
            $U(\lambda x) = \lambda U(x)$
    \end{enumerate}
\end{definition}
\begin{definition}
    Пусть $X_1, X_2, \dots, X_n, Y$ ~--- пространства над одним 
    полем скаляров.
    Тогда $U\colon X_1\times\dots\times X_n\mapsto Y$~--- полилинейное,
    если оно линейно по каждому из аргументов.
\end{definition}
\begin{remark}
    Часто скобочки опускаются: $U(x) = Ux$
\end{remark}
\begin{example}
    \begin{enumerate}
        \item
            $X = C[-1, 1], \delta\colon X\mapsto \R, \delta(f)=f(0)$. 
            Тогда $\delta$~--- линейное отображение.
        \item
        $X = C[a,b], Y = \R$. Тогда 
        $Uf = \bigintss\limits_{a}^{b}{fdx}$ ~---
        линейное отображение.
        \item
            $X = C[a,b], Y = C[a,b]$. Тогда
           $(Uf)(x) = \bigintss\limits_{a}^{x}{f(t)dt}$ 
        \item
            $X = C^1[a,b], Y = C[a,b]$. Тогда 
            $(Df)(x) = f'(x), D\colon X\mapsto Y$ тоже
            линейное отображение.
        \item
            $X_1 = X_2 = \dots = X_n = \R = Y$. Тогда
            $U(x_1,\dots, x_n) = x_1 \ldots x_n$~---
            полилинейное отображение.
        \item
           $X_1 = \R^m, X_2 = \R^m, Y = \R$. Тогда
           $U(X_1, X_2) = (X_1, X_2)$, где $(X_1, X_2)$~---
           скалярное произведение, линейное по первой координате.
        \item
            $X_1 = \R^3, X_2 = \R^3, Y = \R^3$. Тогда
            $U(x_1, x_2) = x_1\times x_2 = [x_1, x_2]$, где
            $[x_1,x_2]$~--- линейное отображение,
            полилинейное отображение.
        \item
            $X_1 = \dots = X_m = \R^m$. Тогда 
            $U(x_1,\dots, x_m) = \det(x_1, \dots, x_m)$ 
            полилинейное отображение.
    \end{enumerate}
\end{example}
\begin{theorem}[О эквивалентных условиях непрерывности]
    $U\colon X\mapsto Y$ ~--- линейное, $X,Y$ ~--- линейные,
    нормированные отображения (далее лно)
    над одним полем скаляров.
    Тогда следующие утверждения эквивалентны:
    \begin{enumerate}
        \item$U$ ~--- непрерывно
        \item $U$ ~--- непрерывно в 0
        \item $\exists C\colon \forall x\in X, \abs{\abs{Ux}}_Y\leqslant C\abs{\abs{x}}_X$
    \end{enumerate}
\end{theorem}
\newpage
\begin{proof}
    \begin{enumerate}
        \item $(1)\Rightarrow (2)$~--- очевидно
        \item  $(2)\Rightarrow (3)$
             $\forall \varepsilon > 0, \exists \delta > 0\colon
             \forall x\colon \norm{x} \leqslant \delta, \norm{Ux} \leqslant \varepsilon$.
             Возьмём произвольное $x$ и сожмём его так, чтобы он лежал 
             в $\delta$ окрестности нуля.
             \[
                 \overline{x} = x\frac{\delta}{\norm{x}} \Rightarrow
                 \left[\norm{\overline{x}} \le \delta\right] 
                 \Rightarrow \norm{U\overline{x}} \leqslant  \varepsilon
                 \Rightarrow \frac{\delta}{\norm{x}}\norm{Ux} \leqslant \varepsilon
                 \Rightarrow \norm{Ux} \le \frac{\varepsilon}{\delta} \norm{x}
             \]
         \item $(3)\Rightarrow(1)$
             % $\|U(x_1) - U(x_2)\| \leq C\|x_1 - x_2\|$~---
             липшецевость  $\Rightarrow$ непрерывное.
    \end{enumerate}
\end{proof}
\begin{theorem}
    $U\colon X_1\times\dots \times X_n\mapsto Y$~--- полилинейное.
    Тогда следующие утверждения эквивалентны:
    \begin{enumerate}
        \item
            $U$ ~--- непрерывно
        \item 
            $U$ ~--- непрерывно в 0
        \item
            $\exists C\colon \norm{U(X_1,\dots, X_n)} \leq
            C\norm{x_1}\cdot\ldots\cdot\norm{x_n}$
    \end{enumerate}
\end{theorem}
\begin{remark}
    Например $\norm{(x_1, \cdots, x_n)}_{x_1 \times \cdots \times x_n} = \norm{x_1}_{x_1} + \cdots + \norm{x_n}_{x_n}$ (или какая-то другая, так как они все эквивалентны в конечномерном пространстве).
\end{remark}
\begin{definition}
    $U: X\rightarrow Y$ ~--- лно.
    $\norm{U} = \inf\{C \mid \forall x\in X \hence \norm{Ux}\leq C\norm{x}\}$
\end{definition}
\begin{remark}
$\inf$ достигается, а именно:
\[
    \forall x\in X, \norm{Ux}\leq \norm{U}\cdot \norm{x} \Rightarrow \norm{U} \geq \frac{\norm{Ux}}{\norm{x}}
\]
\end{remark}
\begin{example}
    $U\colon C[a,b]\mapsto C[a,b]$, причём
    $(Uf)(x) = \bigintss\limits_{a}^{x}{f(t)dt}$.
Хотим оценить, считая, что норма в пространстве непрерывных функций это $\inf\limits_{[a,b]} f$.
    \[
        \int_{a}^{x} f(t)~dt \leqslant \max(f) \cdot (x - a) \Rightarrow
        \norm{Uf} \leqslant (b - a) \norm{f} \hence \norm{U} = b - a
    \]
\end{example}
